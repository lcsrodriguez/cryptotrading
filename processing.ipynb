{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388258de",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf5a3c",
   "metadata": {},
   "source": [
    "## Importing initial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720f1410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:09.448867Z",
     "start_time": "2023-04-07T15:09:08.143301Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53801973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:31.524139Z",
     "start_time": "2023-04-07T15:09:10.200840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing training dataset\n",
    "TRAIN = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TRAIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721288ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:33.798497Z",
     "start_time": "2023-04-07T15:09:31.526619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing second training dataset\n",
    "TRAIN_2 = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TRAIN_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daca9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:39.020802Z",
     "start_time": "2023-04-07T15:09:39.013849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing test dataset\n",
    "TEST = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TEST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d48ea2",
   "metadata": {},
   "source": [
    "## Processing & On-disk saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed52a4",
   "metadata": {},
   "source": [
    "After an in-depth visualization, we have observed that the few samples contained inside the `TEST` dataset are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f837d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:40.577513Z",
     "start_time": "2023-04-07T15:09:40.574663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the continuity of the time series\n",
    "#TRAIN.tail()\n",
    "#TRAIN_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0892988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:09:42.362194Z",
     "start_time": "2023-04-07T15:09:41.201022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging TRAIN and TRAIN_2 as df\n",
    "df = pd.concat([TRAIN, TRAIN_2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb55893",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "- First timestamp: **2018-01-01 00:01:00**\n",
    "- Last timestamp: **2022-01-24 00:00:00**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4793abbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:10:21.587159Z",
     "start_time": "2023-04-07T15:10:03.942942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving on-disk (CSV + Parquet)\n",
    "df_ = df.copy(deep=True)\n",
    "\n",
    "try:\n",
    "    df_.drop([\"Target\"], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Converting the timestamp column\n",
    "df_[\"timestamp\"] = pd.to_datetime(arg=df_[\"timestamp\"], unit=\"s\", errors=\"ignore\")\n",
    "\n",
    "# Setting the timestamp column as index column\n",
    "df_.set_index([\"timestamp\"], inplace=True)\n",
    "\n",
    "# Converting the Count number\n",
    "df_[\"Count\"] = pd.to_numeric(arg=df_[\"Count\"], downcast=\"integer\")\n",
    "\n",
    "# Saving it to a new Parquet file in assets/ (better for file I/O speed & compression)\n",
    "table = pa.Table.from_pandas(df=df_)\n",
    "pq.write_table(table=table, where=f\"assets/parquet/ALL.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ddd83",
   "metadata": {},
   "source": [
    "We are now:\n",
    "1. Splitting our dataframe by `asset_id`\n",
    "2. Removing unnecessary columns\n",
    "3. Converting the timestamp as a pandas `DateTime` object\n",
    "4. Setting this datetime as index\n",
    "5. Performing some additional operations to compress the data and remove some non-expected precision\n",
    "6. Saving to disk as CSV and Parquet files for each `asset_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43444e76",
   "metadata": {},
   "source": [
    "For instance, the column `Count` is an integer-value property by definition.\n",
    "However, our dataset and/or import procedure casted it as a floatting-point number.\n",
    "\n",
    "$\\longrightarrow$ We will reduce this precision using the `.astype()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50c639b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T14:45:30.333745Z",
     "start_time": "2023-04-07T14:42:03.760868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Processing Asset #0 \t (Binance Coin)\n",
      "--> Processing Asset #1 \t (Bitcoin)\n",
      "--> Processing Asset #2 \t (Bitcoin Cash)\n",
      "--> Processing Asset #3 \t (Cardano)\n",
      "--> Processing Asset #4 \t (Dogecoin)\n",
      "--> Processing Asset #5 \t (EOS.IO)\n",
      "--> Processing Asset #6 \t (Ethereum)\n",
      "--> Processing Asset #7 \t (Ethereum Classic)\n",
      "--> Processing Asset #8 \t (IOTA)\n",
      "--> Processing Asset #9 \t (Litecoin)\n",
      "--> Processing Asset #10 \t (Maker)\n",
      "--> Processing Asset #11 \t (Monero)\n",
      "--> Processing Asset #12 \t (Stellar)\n",
      "--> Processing Asset #13 \t (TRON)\n"
     ]
    }
   ],
   "source": [
    "# Hashmap of train datasets for each asset id\n",
    "df_dict = {asset_id: None for asset_id in Utils.ASSET_IDS}\n",
    "\n",
    "# For each asset id, perform the pre-processing\n",
    "for asset_id in Utils.ASSET_IDS:\n",
    "    print(f\"--> Processing Asset #{asset_id} \\t ({Utils.get_asset_name(asset_id)})\")\n",
    "    \n",
    "    # Retrieving the corresponding data rows\n",
    "    df_dict[asset_id] = df[df[\"Asset_ID\"] == asset_id]\n",
    "    \n",
    "    # Removing the Asset_ID column (useless now)\n",
    "    try:\n",
    "        df_dict[asset_id].drop([\"Asset_ID\", \"Target\"], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Converting the timestamp column\n",
    "    df_dict[asset_id][\"timestamp\"] = pd.to_datetime(arg=df_dict[asset_id][\"timestamp\"], \n",
    "                                                     unit=\"s\", \n",
    "                                                     errors=\"ignore\")\n",
    "    # Setting the timestamp column as index column\n",
    "    df_dict[asset_id].set_index([\"timestamp\"], inplace=True)\n",
    "    \n",
    "    # Converting the Count number\n",
    "    df_dict[asset_id][\"Count\"] = pd.to_numeric(arg=df_dict[asset_id][\"Count\"], \n",
    "                                               downcast=\"integer\")\n",
    "\n",
    "    # Saving it to a new CSV file in assets/\n",
    "    df_dict[asset_id].to_csv(path_or_buf=f\"assets/csv/{asset_id}.csv\")\n",
    "    \n",
    "    # Saving it to a new Parquet file in assets/ (better for file I/O speed & compression)\n",
    "    table = pa.Table.from_pandas(df=df_dict[asset_id])\n",
    "    pq.write_table(table=table, where=f\"assets/parquet/{asset_id}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654dc43",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e091a7",
   "metadata": {},
   "source": [
    "We want to check if the sum of the number of rows for each `asset_id` is equal to the number of rows from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba80ffb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T13:53:52.666779Z",
     "start_time": "2023-04-07T13:53:52.663402Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum([df_dict[k].shape[0] for k in Utils.ASSET_IDS]) == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f21f9",
   "metadata": {},
   "source": [
    "We can also check the memory usage of the newly created pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6860c570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T13:54:32.557322Z",
     "start_time": "2023-04-07T13:54:32.528110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mem. usage: \t 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Computing the memory usage of each DataFrame\n",
    "MEM_USAGE = {asset_id: df_dict[asset_id].memory_usage(index=True).sum()/10**6 for asset_id in Utils.ASSET_IDS}\n",
    "\n",
    "# Computing the global memory usage\n",
    "GLOBAL_MEM_USAGE = sum(list(MEM_USAGE.values()))/10**3\n",
    "print(f\"Global mem. usage: \\t {GLOBAL_MEM_USAGE :.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
