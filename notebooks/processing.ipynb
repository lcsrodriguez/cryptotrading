{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388258de",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b45434b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:48:24.970494Z",
     "start_time": "2023-04-07T21:48:24.916878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enabling the notebook execution from this sub-folder\n",
    "import sys, os, ipynbname\n",
    "NOTEBOOK_NAME = f\"{ipynbname.name()}.ipynb\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(NOTEBOOK_NAME), os.path.pardir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720f1410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:48:30.763981Z",
     "start_time": "2023-04-07T21:48:28.840875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importin utils global variables and methods\n",
    "from src.Utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf5a3c",
   "metadata": {},
   "source": [
    "## Importing initial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53801973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:48:55.652218Z",
     "start_time": "2023-04-07T21:48:32.990432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing training dataset\n",
    "TRAIN = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TRAIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721288ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:49:01.529729Z",
     "start_time": "2023-04-07T21:48:59.360214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing second training dataset\n",
    "TRAIN_2 = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TRAIN_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daca9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:49:01.538355Z",
     "start_time": "2023-04-07T21:49:01.532565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing test dataset\n",
    "TEST = pd.read_csv(filepath_or_buffer=Utils.FILENAMES[\"TEST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d48ea2",
   "metadata": {},
   "source": [
    "## Processing & On-disk saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed52a4",
   "metadata": {},
   "source": [
    "After an in-depth visualization, we have observed that the few samples contained inside the `TEST` dataset are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f837d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:49:03.695914Z",
     "start_time": "2023-04-07T21:49:03.692739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the continuity of the time series\n",
    "#TRAIN.tail()\n",
    "#TRAIN_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0892988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:49:06.389492Z",
     "start_time": "2023-04-07T21:49:04.200035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging TRAIN and TRAIN_2 as df\n",
    "df = pd.concat([TRAIN, TRAIN_2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb55893",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "- First timestamp: **2018-01-01 00:01:00**\n",
    "- Last timestamp: **2022-01-24 00:00:00**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4793abbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:49:38.420373Z",
     "start_time": "2023-04-07T21:49:23.465097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving on-disk (CSV + Parquet)\n",
    "SAVE_CSV, SAVE_PQT = False, True\n",
    "\n",
    "# Deep-copying the DataFrame\n",
    "df_ = df.copy(deep=True)\n",
    "\n",
    "# Removing the useless column\n",
    "try:\n",
    "    df_.drop([\"Target\"], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Converting the timestamp column\n",
    "df_[\"timestamp\"] = pd.to_datetime(arg=df_[\"timestamp\"], unit=\"s\", errors=\"ignore\")\n",
    "\n",
    "# Setting the timestamp column as index column\n",
    "df_.set_index([\"timestamp\"], inplace=True)\n",
    "\n",
    "# Converting the Count number\n",
    "df_[\"Count\"] = pd.to_numeric(arg=df_[\"Count\"], downcast=\"integer\")\n",
    "\n",
    "# Checking for +inf/-inf values\n",
    "df_[\"VWAP\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Checking for NaN values\n",
    "nb_NaN_values = df_.isnull().sum().sum()\n",
    "\n",
    "# Replacing the NaN values by the previous value (for columns VWAP & Volume ==> No incidence)\n",
    "if nb_NaN_values != 0: # Expected: 9 for the asset_id: 10\n",
    "    df_.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "if SAVE_CSV:\n",
    "    # Saving it to a new CSV file in assets/\n",
    "    df_.to_csv(path_or_buf=f\"{Utils.ASSETS_FOLDER}/csv/ALL.csv\")\n",
    "\n",
    "if SAVE_PQT:\n",
    "    # Saving it to a new Parquet file in assets/ (better for file I/O speed & compression)\n",
    "    table = pa.Table.from_pandas(df=df_)\n",
    "    pq.write_table(table=table, where=f\"{Utils.ASSETS_FOLDER}/parquet/ALL.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ddd83",
   "metadata": {},
   "source": [
    "We are now:\n",
    "1. Splitting our dataframe by `asset_id`\n",
    "2. Removing unnecessary columns\n",
    "3. Converting the timestamp as a pandas `DateTime` object\n",
    "4. Setting this datetime as index\n",
    "5. Performing some additional operations to compress the data and remove some non-expected precision\n",
    "6. Saving to disk as CSV and Parquet files for each `asset_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43444e76",
   "metadata": {},
   "source": [
    "For instance, the column `Count` is an integer-value property by definition.\n",
    "However, our dataset and/or import procedure casted it as a floatting-point number.\n",
    "\n",
    "$\\longrightarrow$ We will reduce this precision using the `.astype()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c639b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:51:03.537471Z",
     "start_time": "2023-04-07T21:50:26.617693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Processing Asset #0 \t (Binance Coin)\n",
      "--> Processing Asset #1 \t (Bitcoin)\n",
      "--> Processing Asset #2 \t (Bitcoin Cash)\n",
      "--> Processing Asset #3 \t (Cardano)\n",
      "--> Processing Asset #4 \t (Dogecoin)\n",
      "--> Processing Asset #5 \t (EOS.IO)\n",
      "--> Processing Asset #6 \t (Ethereum)\n",
      "--> Processing Asset #7 \t (Ethereum Classic)\n",
      "--> Processing Asset #8 \t (IOTA)\n",
      "--> Processing Asset #9 \t (Litecoin)\n",
      "--> Processing Asset #10 \t (Maker)\n",
      "--> Processing Asset #11 \t (Monero)\n",
      "--> Processing Asset #12 \t (Stellar)\n",
      "--> Processing Asset #13 \t (TRON)\n"
     ]
    }
   ],
   "source": [
    "# Hashmap of train datasets for each asset id\n",
    "df_dict = {asset_id: None for asset_id in Utils.ASSET_IDS}\n",
    "\n",
    "SAVE_CSV, SAVE_PQT = False, True\n",
    "\n",
    "# For each asset id, perform the pre-processing\n",
    "for asset_id in Utils.ASSET_IDS:\n",
    "    print(f\"--> Processing Asset #{asset_id} \\t ({Utils.get_asset_name(asset_id)})\")\n",
    "    \n",
    "    # Retrieving the corresponding data rows\n",
    "    df_dict[asset_id] = df[df[\"Asset_ID\"] == asset_id]\n",
    "    \n",
    "    # Removing the Asset_ID column (useless now)\n",
    "    try:\n",
    "        df_dict[asset_id].drop([\"Asset_ID\", \"Target\"], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Converting the timestamp column\n",
    "    df_dict[asset_id][\"timestamp\"] = pd.to_datetime(arg=df_dict[asset_id][\"timestamp\"], \n",
    "                                                     unit=\"s\", \n",
    "                                                     errors=\"ignore\")\n",
    "    # Setting the timestamp column as index column\n",
    "    df_dict[asset_id].set_index([\"timestamp\"], inplace=True)\n",
    "    \n",
    "    # Converting the Count number\n",
    "    df_dict[asset_id][\"Count\"] = pd.to_numeric(arg=df_dict[asset_id][\"Count\"], \n",
    "                                               downcast=\"integer\")\n",
    "    \n",
    "    # Checking for +inf/-inf values\n",
    "    df_dict[asset_id][\"VWAP\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Checking for NaN values\n",
    "    nb_NaN_values = df_dict[asset_id].isnull().sum().sum()\n",
    "    \n",
    "    # Replacing the NaN values by the previous value (for columns VWAP & Volume ==> No incidence)\n",
    "    if nb_NaN_values != 0: # Expected: 9 for the asset_id: 10\n",
    "        df_dict[asset_id].fillna(method=\"ffill\", inplace=True) # bfill is also available\n",
    "    \n",
    "    if SAVE_CSV:\n",
    "        # Saving it to a new CSV file in assets/\n",
    "        df_dict[asset_id].to_csv(path_or_buf=f\"{Utils.ASSETS_FOLDER}/csv/{asset_id}.csv\")\n",
    "\n",
    "    if SAVE_PQT:\n",
    "        # Saving it to a new Parquet file in assets/ (better for file I/O speed & compression)\n",
    "        table = pa.Table.from_pandas(df=df_dict[asset_id])\n",
    "        pq.write_table(table=table, where=f\"{Utils.ASSETS_FOLDER}/parquet/{asset_id}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654dc43",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e091a7",
   "metadata": {},
   "source": [
    "We want to check if the sum of the number of rows for each `asset_id` is equal to the number of rows from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba80ffb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:51:07.364804Z",
     "start_time": "2023-04-07T21:51:07.359257Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum([df_dict[k].shape[0] for k in Utils.ASSET_IDS]) == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f21f9",
   "metadata": {},
   "source": [
    "We can also check the memory usage of the newly created pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6860c570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T21:51:08.954936Z",
     "start_time": "2023-04-07T21:51:08.935920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mem. usage: \t 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Computing the memory usage of each DataFrame\n",
    "MEM_USAGE = {asset_id: df_dict[asset_id].memory_usage(index=True).sum()/10**6 for asset_id in Utils.ASSET_IDS}\n",
    "\n",
    "# Computing the global memory usage\n",
    "GLOBAL_MEM_USAGE = sum(list(MEM_USAGE.values()))/10**3\n",
    "print(f\"Global mem. usage: \\t {GLOBAL_MEM_USAGE :.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
